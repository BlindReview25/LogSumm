{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"You are an advanced log analysis assistant. Your task is to analyze the given log chunk and generate a concise interpretation.\n",
        "\n",
        "Interpretation Task:\n",
        "Provide a brief interpretation of this log chunk, focusing on critical events (e.g., errors, warnings, performance issues, system malfunctions, potential issues) and their impact on system operations. Keep it concise, avoid redundancy, and exclude irrelevant details. Provide the result in <start></end> tags. If no critical events are found, return: <start>normal</end>.\n",
        "\n",
        "\n",
        "Example:\n",
        "Log chunk:\n",
        "LDAP: Built with OpenLDAP LDAP SDK\n",
        "LDAP: SSL support unavailable\n",
        "suEXEC mechanism enabled (wrapper: /usr/sbin/suexec)\n",
        "Digest: generating secret for digest authentication ...\n",
        "Digest: done\n",
        "\n",
        "Interpretation:\n",
        "<start>LDAP is built with OpenLDAP SDK but lacks SSL support, posing a moderate security risk due to potential unencrypted communication. No other critical issues detected.</end>\n",
        "\n",
        "Now, analyze the following log chunk and provide its interpretation:\n",
        "\n",
        "### Input:\n",
        "Log chunk:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"Chunk\"]\n",
        "\n",
        "    texts = []\n",
        "    for  input in inputs:\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format( input) + EOS_TOKEN\n",
        "        # print(text)\n",
        "        texts.append(text)\n",
        "\n",
        "    return { \"text\" : texts, }\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Miag0znbDLIB"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbVK8bDpDKfu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "def get_summ(response):\n",
        "\n",
        "  pattern = r\"(?<=### Response:\\n)(.*)\"\n",
        "\n",
        "  match = re.search(pattern, response, re.DOTALL)\n",
        "\n",
        "  if match:\n",
        "      extracted_text = match.group(1)  # Extract the first capturing group\n",
        "      # print(extracted_text)\n",
        "      return extracted_text\n",
        "  else:\n",
        "      # print(\"No match found\")\n",
        "      return None\n",
        "\n",
        "\n",
        "def get_response(input):\n",
        "    inputs = tokenizer(\n",
        "        [\n",
        "            alpaca_prompt.format(\n",
        "                input,  # input\n",
        "                \"\",  # output - leave this blank for generation!\n",
        "            )\n",
        "        ], return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=400, use_cache=True)\n",
        "\n",
        "    # print(f\"Input: {input}\" )\n",
        "\n",
        "    full_response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "    print(f\"Full response: {full_response}\" )\n",
        "\n",
        "    extracted_summ = get_summ(full_response[0])\n",
        "\n",
        "    return extracted_summ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import time  # Import the time module\n",
        "\n",
        "\n",
        "input_file = \"/content/test.csv\"\n",
        "output_file = \"/content/test_base.csv\"\n",
        "checkpoint_file = \"/content/test_base_checkpoint.csv\"  # Temp checkpoint file\n",
        "\n",
        "def process_file(input_file, output_file, checkpoint_file):\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    if 'Chunk' not in df.columns:\n",
        "        raise ValueError(\"The input CSV must contain 'Chunk'.\")\n",
        "\n",
        "    # Check if there is a checkpoint file to resume progress\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        df_checkpoint = pd.read_csv(checkpoint_file)\n",
        "        processed_count = len(df_checkpoint[df_checkpoint[\"Apache_base_f1\"] != \"\"])\n",
        "        print(f\"Resuming from checkpoint... {processed_count} rows already processed.\")\n",
        "    else:\n",
        "        df_checkpoint = df.copy()\n",
        "        df_checkpoint[\"Apache_base_f1\"] = \"\"  # Initialize column\n",
        "        processed_count = 0  # Start from the beginning\n",
        "\n",
        "    counter = processed_count\n",
        "    start_time = time.time()  # Start global timer\n",
        "\n",
        "    for index, row in df.iloc[processed_count:].iterrows():  # Resume from last processed row\n",
        "        counter += 1\n",
        "        log_entry = row['Chunk']\n",
        "\n",
        "        print(f\"Counter: {counter}\")\n",
        "        print(f\"Log Chunk: {log_entry}\")\n",
        "\n",
        "        try:\n",
        "            summary = get_response(log_entry)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing log entry: {e}\")\n",
        "            summary = \"ERROR\"\n",
        "\n",
        "        print(f\"Extracted Summary: {summary}\")\n",
        "        df_checkpoint.at[index, \"Apache_base_f1\"] = summary  # Store result\n",
        "        print(\"--------------------------------------------------------\\n\")\n",
        "\n",
        "        # Save progress and print cumulative execution time every 100 entries\n",
        "        if counter % 100 == 0:\n",
        "            df_checkpoint.to_csv(checkpoint_file, index=False)\n",
        "            elapsed = time.time() - start_time\n",
        "            mins, secs = divmod(elapsed, 60)\n",
        "            print(f\"Checkpoint saved at row {counter}\")\n",
        "            print(f\"Cumulative execution time: {int(mins)} min {int(secs)} sec\")\n",
        "\n",
        "    # Final save after all processing\n",
        "    df_checkpoint.to_csv(output_file, index=False)\n",
        "    print(\"Final results saved!\")\n",
        "\n",
        "    # Remove checkpoint file after completion\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        os.remove(checkpoint_file)\n",
        "\n",
        "    # Total execution time\n",
        "    total_time = time.time() - start_time\n",
        "    mins, secs = divmod(total_time, 60)\n",
        "    print(f\"\\n Total execution time: {int(mins)} min {int(secs)} sec\")\n",
        "\n",
        "# Run the processing\n",
        "process_file(input_file, output_file, checkpoint_file)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}