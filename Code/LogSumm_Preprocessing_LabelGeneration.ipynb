{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Chunking"
      ],
      "metadata": {
        "id": "mwomEiiGJRxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def chunk_logs(input_file, output_file, chunk_size=5):\n",
        "    # List to store chunks of log events\n",
        "    log_chunks = []\n",
        "\n",
        "    with open(input_file, 'r', newline='') as csv_in:\n",
        "        reader = csv.reader(csv_in)\n",
        "        header = next(reader)  # Assuming the first row is the header\n",
        "        chunk = []\n",
        "\n",
        "        for row in reader:\n",
        "            chunk.append(row)\n",
        "            if len(chunk) == chunk_size:\n",
        "                log_chunks.append(chunk)  # Add chunk to log_chunks\n",
        "                chunk = []\n",
        "\n",
        "        # Add any remaining log events as the final chunk if they don't fill up the last chunk\n",
        "        if chunk:\n",
        "            log_chunks.append(chunk)\n",
        "\n",
        "    # Write the chunks to a new CSV file\n",
        "    with open(output_file, 'w', newline='') as csv_out:\n",
        "        writer = csv.writer(csv_out)\n",
        "        writer.writerow(['Chunk'])  # Update header for clarity\n",
        "        for chunk in log_chunks:\n",
        "            # Convert each chunk into a single string with rows separated by semicolons\n",
        "            chunk_str = '; '.join([', '.join(event) for event in chunk])\n",
        "            writer.writerow([chunk_str])\n",
        "\n",
        "    return log_chunks\n",
        "\n",
        "# Usage\n",
        "input_csv = '/content/BGL_full.csv'\n",
        "output_csv = '/content/BGL_5.csv'\n",
        "log_chunks = chunk_logs(input_csv, output_csv)\n"
      ],
      "metadata": {
        "id": "DKmTxojeVPVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unique chunk selection"
      ],
      "metadata": {
        "id": "3gNV3HM9NtOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file (replace the path with your actual file location)\n",
        "file_path = \"/content/BGL_full.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop duplicate log chunks and select the first 1000 unique ones\n",
        "unique_chunks = df['Content'].drop_duplicates()\n",
        "\n",
        "# Create a new DataFrame with the unique log chunks\n",
        "unique_df = pd.DataFrame({'Content': unique_chunks})\n",
        "\n",
        "# Save the result to a new CSV file\n",
        "output_path = \"/content/BGL_full_no_duplicate.csv\"\n",
        "unique_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Saved unique log chunks to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJkbPsChNwsv",
        "outputId": "7171fb85-219b-4d9a-a14b-69fdaa64ef55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved unique log chunks to: /content/BGL_full_no_duplicate.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test train selection"
      ],
      "metadata": {
        "id": "TLemRPyv_abP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your CSV file\n",
        "df = pd.read_csv(\"/content/BGL_5_no_duplicate.csv\")  # Replace with the path to your CSV\n",
        "\n",
        "# Randomly sample 200 rows for test set\n",
        "test_df = df.sample(n=1000, random_state=42)\n",
        "\n",
        "# Remaining rows for train set\n",
        "train_df = df.drop(test_df.index)\n",
        "\n",
        "# Save to new CSV files\n",
        "test_df.to_csv(\"BGL_test.csv\", index=False)\n",
        "train_df.to_csv(\"BGL_train.csv\", index=False)\n",
        "\n",
        "print(\"1000 rows saved to test.csv, remaining saved to train.csv\")\n"
      ],
      "metadata": {
        "id": "JrY6nJHw_ZyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Shrink"
      ],
      "metadata": {
        "id": "ZkygrtLiYJv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the training data\n",
        "df = pd.read_csv(\"BGL_train.csv\")\n",
        "\n",
        "# Randomly select 2000 chunks without replacement\n",
        "sampled_df = df.sample(n=2000, random_state=42)  # You can change the seed for different random samples\n",
        "\n",
        "# Save the selected chunks to a new CSV file\n",
        "sampled_df.to_csv(\"BGL_train_2k.csv\", index=False)\n",
        "\n",
        "print(\"2000 random chunks saved to finaltrain.csv\")\n"
      ],
      "metadata": {
        "id": "h8ZVuLTjYL3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ground Truth Generation for Each Level of Chunking"
      ],
      "metadata": {
        "id": "S4Edl2uNYNMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e-bpB5Mw6B4",
        "outputId": "4b6e298f-db87-43e0-afb0-5db67b2b8665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.6.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.20.1)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.87.0\n",
            "    Uninstalling openai-1.87.0:\n",
            "      Successfully uninstalled openai-1.87.0\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Label Generation"
      ],
      "metadata": {
        "id": "S7G54sFt3mGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import openai\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "openai.api_key = \"xxx\"\n",
        "# Function to get summary from GPT-4o for a given chunk\n",
        "def get_summary(chunk_text):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an advanced summarization and log analysis assistant. Your task is to analyze the given log chunk and generate a summary including the critical log events, then provide definitions for important concepts and an interpretation for the log events in summary.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"\n",
        "Task:\n",
        "\n",
        "Analyze the log chunk step-by-step and provide summary, definitions, and interpretation:\n",
        "\n",
        "1) Summary: Extract and return only critical log events indicating errors, failures, system malfunctions, or potential issues. Exclude routine events.\n",
        "\n",
        "2) Definitions: For every concept or term relevant to understanding the critical events or resolving them, provide clear and concise definitions. Prioritize terms that are technical, operational, or diagnostic in nature (e.g., protocols, subsystems, mechanisms).\n",
        "\n",
        "3) Interpretation: Provide a brief interpretation, focusing on critical events present in summary and their impact on system operations. Keep it concise, avoid redundancy, and exclude irrelevant details.\n",
        "\n",
        "If no critical events are found, return: <start>normal</end>.\n",
        "\n",
        "Example:\n",
        "\n",
        "Log chunk:\n",
        "LDAP: Built with OpenLDAP LDAP SDK;LDAP: SSL support unavailable;suEXEC mechanism enabled (wrapper: /usr/sbin/suexec);Digest: generating secret for digest authentication ...Digest: done\n",
        "\n",
        "\n",
        "<start><Summary>LDAP: SSL support unavailable</Summary><Definitions>- LDAP (Lightweight Directory Access Protocol): A protocol used for accessing and managing distributed directory information services over a network.- OpenLDAP: An open-source implementation of the LDAP protocol, providing directory services.- SSL (Secure Sockets Layer): A cryptographic protocol used to provide secure communication over a network.- SSL support unavailable: Indicates that encrypted communication via SSL is not configured or not supported, potentially exposing sensitive data.- suEXEC: A feature of Apache HTTP Server that allows users to run CGI and SSI scripts under user IDs different from the web server user ID.- Digest authentication: A method of HTTP authentication that uses hashing to secure credentials over a network.</Definitions><Interpretation>LDAP communications are unencrypted, exposing sensitive data (e.g., user credentials) to interception, especially on untrusted networks. This could lead to security breaches, undermining system integrity.</Interpretation></end>\n",
        "\n",
        "Now, analyze the following log chunk and provide its summary, definitions, and interpretation:\n",
        "{chunk_text}\n",
        "\"\"\"}\n",
        "        ],\n",
        "        max_tokens=700,\n",
        "        temperature=0.0\n",
        "    )\n",
        "    summary = response.choices[0].message['content'].strip()\n",
        "    return summary\n",
        "\n",
        "# Load the CSV file with chunks\n",
        "input_file = '/content/Linux_train_2k.csv'\n",
        "output_file = '/content/Linux_train_f4.csv'\n",
        "checkpoint_file = '/content/Linux_train_f4_checkpoint.csv'  # New: temporary checkpoint file\n",
        "\n",
        "# Read the file into a DataFrame\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Check if 'Chunk' column exists and create 'GT' column for summaries\n",
        "if 'Chunk' not in df.columns:\n",
        "    raise ValueError(\"Expected column 'Chunk' not found in CSV file.\")\n",
        "\n",
        "# If GT column already exists, keep the old ones; otherwise create it\n",
        "if 'GT' not in df.columns:\n",
        "    df['GT'] = \"\"  # Create a column for summaries\n",
        "\n",
        "# Resume from checkpoint if partially done\n",
        "start_index = 0\n",
        "if df['GT'].notna().any():\n",
        "    completed_indexes = df[df['GT'] != \"\"].index.tolist()\n",
        "    if completed_indexes:\n",
        "        start_index = max(completed_indexes) + 1\n",
        "        print(f\"Resuming from index {start_index}\")\n",
        "\n",
        "# Process each chunk and get the summary\n",
        "for index in range(start_index, len(df)):\n",
        "    row = df.iloc[index]\n",
        "    chunk_text = row['Chunk']\n",
        "    print(f\"\\nProcessing index {index}:\\n{chunk_text}\\n\")\n",
        "\n",
        "    try:\n",
        "        summary = get_summary(chunk_text)\n",
        "        df.at[index, 'GT'] = summary  # Store the summary in the 'GT' column\n",
        "        print(f\"Summary:\\n{summary}\")\n",
        "        print(f\"------------------------index= {index} -------------------------------\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error at index {index}: {e}\")\n",
        "        print(\"Waiting for 10 seconds before retrying...\")\n",
        "        time.sleep(5)\n",
        "        continue  # Skip this entry or retry later\n",
        "\n",
        "    # Save checkpoint every 10 entries\n",
        "    if (index + 1) % 100 == 0:\n",
        "        df.to_csv(checkpoint_file, index=False)\n",
        "        print(f\"Checkpoint saved at index {index}.\")\n",
        "\n",
        "# Final save\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Final summaries saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "Bc9VrGMP3lqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Post processing (remove newlines and Summary:)**"
      ],
      "metadata": {
        "id": "HFu6jHyOacH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Function to remove newlines and \"Summary:\" in a specific column of a CSV file\n",
        "def remove_newlines_and_summary_in_column(input_file, output_file, column_name):\n",
        "    with open(input_file, 'r', newline='', encoding='utf-8') as infile:\n",
        "        reader = csv.DictReader(infile)\n",
        "\n",
        "        # Get field names from the input file\n",
        "        fieldnames = reader.fieldnames\n",
        "\n",
        "        # Open the output file to write the updated data\n",
        "        with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
        "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "\n",
        "            # Process each row in the input file\n",
        "            for row in reader:\n",
        "                if column_name in row:\n",
        "\n",
        "                    row[column_name] = row[column_name].replace('\\n', ' ').replace('</end>\\n', '</end>').replace('\\r', ' ').replace('Summary:', '').strip()\n",
        "                writer.writerow(row)\n",
        "\n",
        "# Specify input and output file paths\n",
        "input_csv = '/content/Linux_train_f4.csv'   # Replace with your input CSV file path\n",
        "output_csv = '/content/Linux_train_f4_cleaned.csv' # Replace with your desired output CSV file path\n",
        "\n",
        "\n",
        "column_to_clean = 'GT'\n",
        "\n",
        "# Call the function\n",
        "remove_newlines_and_summary_in_column(input_csv, output_csv, column_to_clean)\n",
        "\n",
        "print(f\"Newlines and 'Summary:' removed in column '{column_to_clean}' and saved to {output_csv}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc4r0g6vXhGp",
        "outputId": "37e70742-58b9-4d85-85d4-0d1f8c3c1344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Newlines and 'Summary:' removed in column 'GT' and saved to /content/Linux_train_f4_cleaned.csv.\n"
          ]
        }
      ]
    }
  ]
}