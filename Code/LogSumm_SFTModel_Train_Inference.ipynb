{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcerSSo7l84A"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "]  # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "\n",
        "    model_name=\"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
        "    # model_name=\"mrbmaryam/SFT_F3\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "9LmohFnLmAmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # We support rank stabilized LoRA\n",
        "    loftq_config=None,  # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx5XAU_QmFN2",
        "outputId": "41315424-f474-458a-b686-9de87cbdd5dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.6.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"You are an advanced summarization and log analysis assistant. Your task is to analyze the given log chunk, summarize critical log events and provide a concise interpretation in the specified format.\n",
        "\n",
        "Summarization and Interpretation Task:\n",
        "Analyze the provided log chunk and summarize critical log events indicating errors, failures, malfunctions, or potential issues. Exclude routine events and deduplicate redundant entries. In case of redundancy, only return one of them. For each critical event, provide a severity rating (0-5), brief root cause, potential issues, system impact, and solution. Use this format for results:\n",
        "\n",
        "<start>[event: <log event>] [severity: <0-5>] [root cause: <brief explanation>] [potential issues: <brief description>] [impact: <brief system impact>] [solution: <brief resolution>]</end>\n",
        "\n",
        "If no critical events are found, return: <start>normal</end>.\n",
        "\n",
        "### Input:\n",
        "Log chunk:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "Interpretation:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "\n",
        "    inputs       = examples[\"Chunk\"]\n",
        "    outputs      = examples[\"GT\"]\n",
        "    texts = []\n",
        "    for  input, output in zip(inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mrbmaryam/Train_F3\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "C-gV03qemImN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[5][\"text\"]"
      ],
      "metadata": {
        "id": "vyvjURpFmP7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "BIWcAyfSzeDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "id": "MBwaAs6OmbUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "4C10h2rhNqDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"You are an advanced summarization and log analysis assistant. Your task is to analyze the given log chunk, summarize critical log events and provide a concise interpretation in the specified format.\n",
        "\n",
        "Summarization and Interpretation Task:\n",
        "Analyze the provided log chunk and summarize critical log events indicating errors, failures, malfunctions, or potential issues. Exclude routine events and deduplicate redundant entries. In case of redundancy, only return one of them. For each critical event, provide a severity rating (0-5), brief root cause, potential issues, system impact, and solution. Use this format for results:\n",
        "\n",
        "<start>[event: <log event>] [severity: <0-5>] [root cause: <brief explanation>] [potential issues: <brief description>] [impact: <brief system impact>] [solution: <brief resolution>]</end>\n",
        "\n",
        "If no critical events are found, return: <start>normal</end>.\n",
        "\n",
        "Now, analyze the following log chunk and provide its summary and interpretation:\n",
        "### Input:\n",
        "Log chunk:\n",
        "{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "waZXj0la6ix4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "def get_summ(response):\n",
        "\n",
        "  pattern = r\"(?<=### Response:\\nSummarization and Interpretation:\\n)(.*)\"\n",
        "\n",
        "\n",
        "  match = re.search(pattern, response, re.DOTALL)\n",
        "\n",
        "  if match:\n",
        "      extracted_text = match.group(1)  # Extract the first capturing group\n",
        "      # print(extracted_text)\n",
        "      return extracted_text\n",
        "  else:\n",
        "      # print(\"No match found\")\n",
        "      return None\n",
        "\n",
        "\n",
        "def get_response(input):\n",
        "    inputs = tokenizer(\n",
        "        [\n",
        "            alpaca_prompt.format(\n",
        "                input,  # input\n",
        "                \"\",  # output - leave this blank for generation!\n",
        "            )\n",
        "        ], return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=800, use_cache=True)\n",
        "\n",
        "    full_response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "    print(f\"Full response: {full_response}\" )\n",
        "\n",
        "    extracted_summ = get_summ(full_response[0])\n",
        "\n",
        "    return extracted_summ"
      ],
      "metadata": {
        "id": "nQH_tGlymjNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import time  # Import time module\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "input_file = \"/content/OpenStack_test_f3.csv\"\n",
        "output_file = \"/content/Test_OpenStack_F3_SFT.csv\"\n",
        "checkpoint_file = \"/content/Test_OpenStack_F3_SFT_checkpoint.csv\"  # Temp checkpoint file\n",
        "\n",
        "def process_file(input_file, output_file, checkpoint_file):\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    if 'Chunk' not in df.columns:\n",
        "        raise ValueError(\"The input CSV must contain 'Chunk'.\")\n",
        "\n",
        "    # Check if there is a checkpoint file to resume progress\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        df_checkpoint = pd.read_csv(checkpoint_file)\n",
        "        processed_count = len(df_checkpoint[df_checkpoint[\"Mistral_sft_f3\"] != \"\"])\n",
        "        print(f\"Resuming from checkpoint... {processed_count} rows already processed.\")\n",
        "    else:\n",
        "        df_checkpoint = df.copy()\n",
        "        df_checkpoint[\"Mistral_sft_f3\"] = \"\"  # Initialize column\n",
        "        processed_count = 0  # Start from the beginning\n",
        "\n",
        "    counter = processed_count\n",
        "\n",
        "    for index, row in df.iloc[processed_count:].iterrows():  # Resume from last processed row\n",
        "        counter += 1\n",
        "        log_entry = row['Chunk']\n",
        "\n",
        "        print(f\"Counter: {counter}\")\n",
        "        print(f\"Log Chunk: {log_entry}\")\n",
        "\n",
        "        try:\n",
        "            summary = get_response(log_entry)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing log entry: {e}\")\n",
        "            summary = \"ERROR\"\n",
        "\n",
        "        print(f\"Extracted Summary: {summary}\")\n",
        "        df_checkpoint.at[index, \"Mistral_sft_f3\"] = summary  # Store result\n",
        "        print(\"--------------------------------------------------------\\n\")\n",
        "\n",
        "        # Save progress every 100 entries\n",
        "        if counter % 100 == 0:\n",
        "            df_checkpoint.to_csv(checkpoint_file, index=False)\n",
        "            print(f\"Checkpoint saved at row {counter}\")\n",
        "            elapsed = time.time() - start_time\n",
        "            mins, secs = divmod(elapsed, 60)\n",
        "            print(f\"Cumulative execution time: {int(mins)} min {int(secs)} sec\")\n",
        "\n",
        "    # Final save after all processing\n",
        "    df_checkpoint.to_csv(output_file, index=False)\n",
        "    print(\"Final results saved!\")\n",
        "\n",
        "    # Remove checkpoint file after completion\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        os.remove(checkpoint_file)\n",
        "\n",
        "# --- Timing Starts Here ---\n",
        "start_time = time.time()\n",
        "\n",
        "process_file(input_file, output_file, checkpoint_file)\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nTotal execution time: {execution_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "zTkwnPvEmmni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Model"
      ],
      "metadata": {
        "id": "6_AwXcqiN339"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = getpass.getpass(\"Token: \")\n",
        "assert os.environ[\"HUGGING_FACE_HUB_TOKEN\"]"
      ],
      "metadata": {
        "id": "0nLiKzzVmsj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7925b5-8b92-48ff-cb8a-4ef1fe74bd01"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Assuming you have already trained your model and tokenizer\n",
        "model_name = \"xxx\"\n",
        "token = \"xxx\"\n",
        "\n",
        "# Push model to Hugging Face Hub\n",
        "model.push_to_hub(model_name)\n",
        "\n",
        "# Push tokenizer to Hugging Face Hub\n",
        "tokenizer.push_to_hub(model_name)\n",
        "\n",
        "print(f\"Model and tokenizer successfully pushed to {model_name} on Hugging Face Hub.\")\n",
        "\n",
        "\"\"\"**Evaluation**\"\"\""
      ],
      "metadata": {
        "id": "AbaCdfjemwTg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}